---
layout: post
title: "R-Squared"
date: 2020-02-12 16:00:00 +0800
tags: [statistics, machine-learning]
# img: "/assets/img/machine.jpg"
description: explore the impact on model with or without cross validation
---

## What is R-Squared

R<sup>2</sup> measures the proportion of the variance for a target that can be explained by the input (or features).
Often used on regression model.
Also known as **coefficient of determination**

R<sup>2</sup> = [ Variance(mean) - Variance(best-model) ] / Variance(mean)

or :

R<sup>2</sup> = 1 - [ Variance(mean) - Variance(best-model) ]

There are a few steps to calculate the R<sup>2</sup> of a regression model:
1. Fit a line to the data, the best model of linear regression
2. Calculate variance(mean), which is the sum of squared-error between target and mean (&mu;)
3. Calculate variance(best-model), which is the sum of squared-error between target and the predicted value using the best-model

R<sup>2</sup> ranges from 0 to 1, because the Variance(best-model) should not perform worse than the mean.

## Interpretation
R<sup>2</sup> = 0 means the regression model performs the same as average.

R<sup>2</sup> = 0.9 means 90% of the variance of target can be explained by the model. That's pretty good!!

There are a few other points I read from different blogs that I don't quite understand. To be claried later:
- R<sup>2</sup> works well with single variable regression model, but not for multi-variables regression. Is it true? [link](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
- what is adjusted R<sup>2</sup>? [link](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/)
- high R<sup>2</sup> not always mean a good fit, likewise low R<sup>2</sup> is not always bad. Is it true? Why?


Reference: [R-squared clearly explained by StatQuest](https://www.youtube.com/watch?v=2AQKmw14mHM)

